{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCC11 - Introduction to Machine Learning, Fall 2022, Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Shawn Santhoshgeorge (1006094673) \\\n",
    "Anaqi Amir Razif (1005813880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Width  Height  Orange\n",
      "0      4       4       1\n",
      "1      6       4       1\n",
      "2      6       5       1\n",
      "3      6       8       0\n",
      "4      6      10       0\n",
      "5      8       8       1\n",
      "6      8      10       0\n"
     ]
    }
   ],
   "source": [
    "# Import the Data and Setup X and y for Training\n",
    "df_train = pd.DataFrame({\n",
    "    \"Width\": [4, 6, 6, 6 , 6, 8 , 8],\n",
    "    \"Height\": [4, 4, 5, 8, 10, 8, 10],\n",
    "    \"Orange\": [1, 1, 1, 0, 0, 1, 0]\n",
    "})\n",
    "\n",
    "print(df_train)\n",
    "\n",
    "# Split Data into X and Y\n",
    "X_train = df_train[['Width', 'Height']].to_numpy()\n",
    "X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1)))) # Add Column of 1's for the bias term\n",
    "Y_train = df_train['Orange'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the corresponding optimization problem in terms of the data provided above and specify the parameters to be estimated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization problem we are trying to solve is the following\n",
    "\n",
    "Model: $P(\\text{Orange}  | \\mathbf{X}) = \\frac{1}{1 + e^{\\mathbf{w}^T \\mathbf{X}}}$, where $\\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ b \\end{bmatrix}$ and $\\mathbf{X} = \\begin{bmatrix} \\text{Weight} \\\\ \\text{Height} \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "Given $\\{x_i, y_i\\}_{i=1, \\cdots, N}$.To find the estimation for the model parameters we would want to minimize the negative log-likelihood as follows\n",
    "\n",
    "$L(\\mathbf{w}) = - \\sum_{i=1}^N y_ilog(P(\\text{Orange} |x_i)) + (1- y_i)log(1 - P(\\text{Orange} |x_i))$\n",
    "\n",
    "After taking the partial derivatives for each $\\mathbf{w}_i$ we get the following $\\frac{\\partial}{\\partial\\mathbf{w}}L(\\mathbf{w}) = -\\sum_{i=1}^N (y_i - p_i)x_i $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(values):\n",
    "    \"\"\"\n",
    "    Return the value from the Sigmoid Function\n",
    "\n",
    "    Args:\n",
    "        - values (ndarray (Shape: (N, 1))): Result of the Dot Product with Model Parameters and Input (w^Tx)\n",
    "\n",
    "    Output:\n",
    "        Values from the Sigmoid Function\n",
    "    \"\"\"\n",
    "\n",
    "    'Checks if values is an array'\n",
    "    assert isinstance(values, np.ndarray), 'values must be an ndarray of Nx1'\n",
    "\n",
    "    return 1 / (1 + np.exp(-values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Parameters:  [ 0.3 -0.2  0.7]\n",
      "After Optimization:  [ 0.27208574 -0.35574855  0.69978802]\n",
      "Train Data Result:  [1 1 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "w = np.asarray([0.3,-0.2, 0.7]) # Initial Weights\n",
    "STEP_SIZE = 0.01\n",
    "\n",
    "def train(x, y, init_w, iters=3):\n",
    "    \"\"\"\n",
    "    Finds the model parameter estimations using Gradient Descent\n",
    "\n",
    "    Args:\n",
    "        - x: (ndarray (Shape: (N, 3))): A Nx3 matrix corresponding to the inputs and 1's.\n",
    "        - y: (ndarray (Shape: (N, 1))): A N-column vector corresponding to the outputs given the inputs.\n",
    "        - init_w: (ndarray (Shape: (3, 1))): Initial Weights and Bias Term for the model\n",
    "        - iters (int): Number of iterations for the Gradient Descent Algorithm (Default=3)\n",
    "\n",
    "    Output:\n",
    "        - w: (ndarray (Shape: (3, 1))): Estimated Weights and Bias Term for the model\n",
    "    \"\"\"\n",
    "\n",
    "    # Creates a copy of the initial weights\n",
    "    w = np.copy(init_w)\n",
    "\n",
    "    # Calculates the gradient and moves the weight closer to the estimate\n",
    "    for _ in range(iters):\n",
    "        deltaW = np.dot(x.T, (sigmoid(np.dot(x, w)) - y))\n",
    "        w -= STEP_SIZE * deltaW\n",
    "\n",
    "    return w\n",
    "\n",
    "# Model Parameter Optimization\n",
    "print(\"Initial Model Parameters: \", w)\n",
    "w = train(X_train, Y_train, w)\n",
    "print(\"After Optimization: \", w)\n",
    "\n",
    "# Model Testing on Training Data\n",
    "Y_train_pred = sigmoid(np.dot(X_train, w)) >= 1/2\n",
    "print(\"Train Data Result: \", 1 * Y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Result:  [1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Import the Data and Setup X for Testing\n",
    "X_test = np.array([(3,3), (4, 10), (9, 8), (9, 10)])\n",
    "X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1))))\n",
    "Y_test_pred = sigmoid(np.dot(X_test, w)) >= 1/2\n",
    "print(\"Test Data Result: \", 1 * Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the new points fit into the following class\n",
    "\n",
    "<center>\n",
    "\n",
    "| Width \t| Height \t| Orange \t|\n",
    "|-------\t|--------\t|--------\t|\n",
    "| 3     \t| 3      \t| 1      \t|\n",
    "| 4     \t| 10     \t| 0      \t|\n",
    "| 9     \t| 8      \t| 1      \t|\n",
    "| 9     \t| 10     \t| 0      \t|\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss one advantage of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of Logsitic Regression is that it has less model parameters compared to another classifer like Gaussian Class Conditionals, this means the training phase will be relatively quick to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Briefly explain whether Logistic Regressionis discriminative or generative ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a discriminative model, since it does not attempt to model the complete probaility of the training data instead it only attempts to model the conditional probability of the target output given the input, for in this case $P(\\text{Orange} | X)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cscc11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85bcffb2f92e5d976c05390d637263682ff066627114711b98042e6877e4a14e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
